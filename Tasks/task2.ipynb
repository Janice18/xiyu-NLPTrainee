{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata,re,math,random\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示数据集的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对数据集作描述性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "      <td>156060.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4079.732744</td>\n",
       "      <td>2.063578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>45050.785842</td>\n",
       "      <td>2502.764394</td>\n",
       "      <td>0.893832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39015.750000</td>\n",
       "      <td>1861.750000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>78030.500000</td>\n",
       "      <td>4017.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>117045.250000</td>\n",
       "      <td>6244.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156060.000000</td>\n",
       "      <td>8544.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            PhraseId     SentenceId      Sentiment\n",
       "count  156060.000000  156060.000000  156060.000000\n",
       "mean    78030.500000    4079.732744       2.063578\n",
       "std     45050.785842    2502.764394       0.893832\n",
       "min         1.000000       1.000000       0.000000\n",
       "25%     39015.750000    1861.750000       2.000000\n",
       "50%     78030.500000    4017.000000       2.000000\n",
       "75%    117045.250000    6244.000000       3.000000\n",
       "max    156060.000000    8544.000000       4.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看评分的类别分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "uniqueLabel=set(train_df['Sentiment'])\n",
    "x=[]\n",
    "y=[]\n",
    "for i in uniqueLabel:\n",
    "    x.append(i)\n",
    "    y.append(train_df['Sentiment'][train_df['Sentiment']==i].size)\n",
    "plt.figure(111)\n",
    "plt.bar(x,y)\n",
    "plt.xlabel('type of review  ')\n",
    "plt.ylabel('count')\n",
    "plt.title('Movie Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一步，定义对数据规范化的函数\n",
    "def remove_non_ascii(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#将数据集中的word全部小写\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#去除标点符号\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#去除数据集中的数字\n",
    "def remove_numbers(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "#去除停用词\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english') and len(word)>1:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "#整合上面的所有定义函数到该函数，来规范化数据\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 切分Phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [A, series, of, escapades, demonstrating, the,...\n",
       "1    [A, series, of, escapades, demonstrating, the,...\n",
       "2                                          [A, series]\n",
       "3                                                  [A]\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第二步，tokenize Phrase\n",
    "train_df['Words'] = train_df['Phrase'].apply(nltk.word_tokenize)\n",
    "train_df['Words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [series, escapades, demonstrating, adage, good...\n",
       "1    [series, escapades, demonstrating, adage, good...\n",
       "2                                             [series]\n",
       "3                                                   []\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第三步 - 应用之前定义的函数\n",
    "train_df['Words'] = train_df['Words'].apply(normalize) \n",
    "train_df['Words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16060\n",
      "16060\n"
     ]
    }
   ],
   "source": [
    "# 第三步 - 创建要用作编码字典的唯一单词列表\n",
    "word_set = set()\n",
    "for l in train_df['Words']:\n",
    "    for word in l:\n",
    "        word_set.add(word)\n",
    "        \n",
    "word_to_int = {word: i for i, word in enumerate(word_set, 1)}\n",
    "\n",
    "# 检查两者长度是否一致\n",
    "print(len(word_set))\n",
    "print(len(word_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [3600, 14627, 14006, 3553, 2215, 14682, 9712, ...\n",
       "1              [3600, 14627, 14006, 3553, 2215, 14682]\n",
       "2                                               [3600]\n",
       "3                                                   []\n",
       "4                                               [3600]\n",
       "Name: Tokens, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据word_to_int去标记每个word\n",
    "train_df['Tokens'] = train_df['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "train_df['Tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# 第四步，获取句子的最长长度\n",
    "max_len = train_df['Tokens'].str.len().max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3600 14627 14006  3553  2215 14682  9712  2215 10302  7493 14584  5219\n",
      "   9128   945 10762     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 3600 14627 14006  3553  2215 14682     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [ 3600     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "all_tokens = np.array([t for t in train_df['Tokens']])\n",
    "encoded_labels = np.array([l for l in train_df['Sentiment']])\n",
    "\n",
    "# 创建数据集长度×最长句子长度的零矩阵\n",
    "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# 对于每个句子，用0补齐维度到max_len\n",
    "for i, row in enumerate(all_tokens):\n",
    "    features[i, :len(row)] = row\n",
    "\n",
    "#打印出特征矩阵的前三行 \n",
    "print(features[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 切分data 为训练集、验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(124848, 30) \n",
      "Validation set: \t(15606, 30) \n",
      "Test set: \t\t(15606, 30)\n"
     ]
    }
   ],
   "source": [
    "train_x,remaing_x,train_y,remaing_y = train_test_split(features,encoded_labels,test_size=0.2,random_state=3)\n",
    "val_x,test_x,val_y,test_y = train_test_split(remaing_x, remaing_y,test_size=0.5)\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "     \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders和批处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）创建一种用于访问数据的已知格式，使用TensorDataset接收输入数据集和具有相同第一维的目标数据集，并创建数据集。\n",
    "\n",
    "2）创建DataLoaders并批量训练，验证Tensor数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "289\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# 创建Tensor数据集\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# 数据载入\n",
    "batch_size = 54\n",
    "\n",
    "# 确保shuffle你的数据集\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# 检查载入数据集的大小(有多少个batches)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建深层网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）首先，我们将文字传递给嵌入层。我们需要一个嵌入层，因为我们有数千个单词，因此我们需要比单热编码向量更有效的输入数据表示。在这种情况下，嵌入层用于降低维数，而不是用于学习语义表示。\n",
    "\n",
    "2）输入字传递到嵌入层后，新的嵌入将传递给LSTM单元。 LSTM单元将添加到网络的循环连接，并使我们能够包含有关电影评论数据中单词序列的信息。 LSTM接收input_size，hidden_​​dim，多个层，丢失概率（用于多个层之间的丢失）和batch_first参数。\n",
    "\n",
    "3）最后，LSTM输出将进入线性层进行最终分类，其输出将被传递到交叉熵损失函数以获得每个预测类的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层次如下：\n",
    "\n",
    "嵌入层，用于将单词标记（整数）转换为特定大小的嵌入。\n",
    "\n",
    "由hidden_state大小和层数定义的LSTM层\n",
    "\n",
    "一个完全连接的输出层，用于将LSTM层输出映射到所需的output_size\n",
    "\n",
    "稍后将通过Crossentropy损失函数应用softmax，其将所有输出转换为概率\n",
    "\n",
    "大多数情况下，网络将具有更多层的更好性能; 2-3之间。 添加更多层允许网络学习真正复杂的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 首先检查GPU是否可用\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM 层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear层\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # 将lstm输出转换为线性层的输入大小\n",
    "        lstm_out = lstm_out.transpose(0,1)\n",
    "        lstm_out = lstm_out[-1]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)        \n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # 创建两个大小为n_layers  x  batch_size  x  hidden_dim的新张量\n",
    "        # 对于LSTM的隐藏状态和单元状态，初始化为零\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Instantiate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们将实例化网络。 首先，定义超参数。\n",
    "\n",
    "vocab_size：我们词汇表的大小或我们输入的单词标记的值范围。\n",
    "\n",
    "output_size：我们所需输出的大小; 我们想要输出的得分（0..4）。\n",
    "\n",
    "embedding_dim：嵌入查找表中的列数; 嵌入的大小。\n",
    "\n",
    "hidden_dim：LSTM单元隐藏层中的单元数。 通常更大是更好的性能。 常用值为128,256,512等。\n",
    "\n",
    "n_layers：网络中LSTM层的数量。 通常在1-3之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(16061, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型（超参）\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 5\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练惯例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是典型的训练代码。 将使用Crossentropy损失，因为这是多类别分类问题。 我们还有一些数据和培训参数：\n",
    "\n",
    "lr：我们的优化器的学习率。\n",
    "\n",
    "epochs：迭代训练数据集的次数。\n",
    "\n",
    "clip：要剪切的最大渐变值（以防止爆炸渐变）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失和优化函数\n",
    "lr=0.003\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 1.165778... Val Loss: 1.296873\n",
      "Epoch: 1/3... Step: 200... Loss: 1.446933... Val Loss: 1.293438\n",
      "Epoch: 1/3... Step: 300... Loss: 1.182896... Val Loss: 1.287417\n",
      "Epoch: 1/3... Step: 400... Loss: 1.184239... Val Loss: 1.288608\n",
      "Epoch: 1/3... Step: 500... Loss: 1.295944... Val Loss: 1.290675\n",
      "Epoch: 1/3... Step: 600... Loss: 1.355662... Val Loss: 1.288833\n",
      "Epoch: 1/3... Step: 700... Loss: 1.175606... Val Loss: 1.290583\n",
      "Epoch: 1/3... Step: 800... Loss: 1.250186... Val Loss: 1.288467\n",
      "Epoch: 1/3... Step: 900... Loss: 1.259532... Val Loss: 1.286385\n",
      "Epoch: 1/3... Step: 1000... Loss: 1.210624... Val Loss: 1.286427\n",
      "Epoch: 1/3... Step: 1100... Loss: 1.125279... Val Loss: 1.284166\n",
      "Epoch: 1/3... Step: 1200... Loss: 1.332783... Val Loss: 1.284108\n",
      "Epoch: 1/3... Step: 1300... Loss: 1.115238... Val Loss: 1.268831\n",
      "Epoch: 1/3... Step: 1400... Loss: 1.126952... Val Loss: 1.226120\n",
      "Epoch: 1/3... Step: 1500... Loss: 1.208673... Val Loss: 1.224395\n",
      "Epoch: 1/3... Step: 1600... Loss: 1.263023... Val Loss: 1.201176\n",
      "Epoch: 1/3... Step: 1700... Loss: 1.368143... Val Loss: 1.206613\n",
      "Epoch: 1/3... Step: 1800... Loss: 1.163541... Val Loss: 1.179148\n",
      "Epoch: 1/3... Step: 1900... Loss: 1.082158... Val Loss: 1.177383\n",
      "Epoch: 1/3... Step: 2000... Loss: 1.138102... Val Loss: 1.180724\n",
      "Epoch: 1/3... Step: 2100... Loss: 1.038769... Val Loss: 1.155815\n",
      "Epoch: 1/3... Step: 2200... Loss: 1.466211... Val Loss: 1.152329\n",
      "Epoch: 1/3... Step: 2300... Loss: 1.140004... Val Loss: 1.136978\n",
      "Epoch: 2/3... Step: 2400... Loss: 1.241623... Val Loss: 1.130505\n",
      "Epoch: 2/3... Step: 2500... Loss: 1.139185... Val Loss: 1.115098\n",
      "Epoch: 2/3... Step: 2600... Loss: 0.841851... Val Loss: 1.097197\n",
      "Epoch: 2/3... Step: 2700... Loss: 1.150474... Val Loss: 1.096843\n",
      "Epoch: 2/3... Step: 2800... Loss: 1.053419... Val Loss: 1.095969\n",
      "Epoch: 2/3... Step: 2900... Loss: 1.058036... Val Loss: 1.071108\n",
      "Epoch: 2/3... Step: 3000... Loss: 1.060135... Val Loss: 1.069192\n",
      "Epoch: 2/3... Step: 3100... Loss: 1.033728... Val Loss: 1.057305\n",
      "Epoch: 2/3... Step: 3200... Loss: 0.815400... Val Loss: 1.051553\n",
      "Epoch: 2/3... Step: 3300... Loss: 0.966967... Val Loss: 1.028571\n",
      "Epoch: 2/3... Step: 3400... Loss: 1.003269... Val Loss: 1.020095\n",
      "Epoch: 2/3... Step: 3500... Loss: 0.989915... Val Loss: 1.020332\n",
      "Epoch: 2/3... Step: 3600... Loss: 0.967475... Val Loss: 1.012263\n",
      "Epoch: 2/3... Step: 3700... Loss: 1.024265... Val Loss: 0.996548\n",
      "Epoch: 2/3... Step: 3800... Loss: 0.839722... Val Loss: 0.996925\n",
      "Epoch: 2/3... Step: 3900... Loss: 1.133408... Val Loss: 0.980756\n",
      "Epoch: 2/3... Step: 4000... Loss: 0.993645... Val Loss: 0.976443\n",
      "Epoch: 2/3... Step: 4100... Loss: 0.882754... Val Loss: 0.970499\n",
      "Epoch: 2/3... Step: 4200... Loss: 1.077936... Val Loss: 0.969271\n",
      "Epoch: 2/3... Step: 4300... Loss: 1.015919... Val Loss: 0.972355\n",
      "Epoch: 2/3... Step: 4400... Loss: 0.921549... Val Loss: 0.956910\n",
      "Epoch: 2/3... Step: 4500... Loss: 1.068740... Val Loss: 0.956531\n",
      "Epoch: 2/3... Step: 4600... Loss: 0.929217... Val Loss: 0.949860\n",
      "Epoch: 3/3... Step: 4700... Loss: 0.854554... Val Loss: 0.951159\n",
      "Epoch: 3/3... Step: 4800... Loss: 0.842633... Val Loss: 0.943952\n",
      "Epoch: 3/3... Step: 4900... Loss: 0.871388... Val Loss: 0.962082\n",
      "Epoch: 3/3... Step: 5000... Loss: 0.891677... Val Loss: 0.936935\n",
      "Epoch: 3/3... Step: 5100... Loss: 0.851077... Val Loss: 0.947449\n",
      "Epoch: 3/3... Step: 5200... Loss: 0.743792... Val Loss: 0.947672\n",
      "Epoch: 3/3... Step: 5300... Loss: 0.893295... Val Loss: 0.937801\n",
      "Epoch: 3/3... Step: 5400... Loss: 0.975824... Val Loss: 0.936301\n",
      "Epoch: 3/3... Step: 5500... Loss: 0.735075... Val Loss: 0.927449\n",
      "Epoch: 3/3... Step: 5600... Loss: 0.960689... Val Loss: 0.921855\n",
      "Epoch: 3/3... Step: 5700... Loss: 1.098999... Val Loss: 0.921736\n",
      "Epoch: 3/3... Step: 5800... Loss: 0.892254... Val Loss: 0.940979\n",
      "Epoch: 3/3... Step: 5900... Loss: 0.917806... Val Loss: 0.914895\n",
      "Epoch: 3/3... Step: 6000... Loss: 0.837873... Val Loss: 0.925003\n",
      "Epoch: 3/3... Step: 6100... Loss: 0.712779... Val Loss: 0.912717\n",
      "Epoch: 3/3... Step: 6200... Loss: 0.922459... Val Loss: 0.901315\n",
      "Epoch: 3/3... Step: 6300... Loss: 1.046551... Val Loss: 0.920218\n",
      "Epoch: 3/3... Step: 6400... Loss: 0.766216... Val Loss: 0.907879\n",
      "Epoch: 3/3... Step: 6500... Loss: 0.886915... Val Loss: 0.900128\n",
      "Epoch: 3/3... Step: 6600... Loss: 0.783118... Val Loss: 0.894575\n",
      "Epoch: 3/3... Step: 6700... Loss: 0.970744... Val Loss: 0.908935\n",
      "Epoch: 3/3... Step: 6800... Loss: 0.979367... Val Loss: 0.892557\n",
      "Epoch: 3/3... Step: 6900... Loss: 0.735619... Val Loss: 0.892962\n"
     ]
    }
   ],
   "source": [
    "# 训练参数\n",
    "epochs = 3 # 3-4 是经观察所得训练集损失停止减少的地方\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 #梯度裁剪\n",
    "\n",
    "# 如果GPU可获得的话，把模型放到GPU上运行\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# 训练一些epochs\n",
    "for e in range(epochs):\n",
    "    # 初始化隐藏状态\n",
    "    h = net.init_hidden(batch_size)\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # 为隐藏状态创建新变量，否则\n",
    "        # 我们通过整个训练历史过程进行反向传播\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # 优化之前，先将梯度归0\n",
    "        net.zero_grad()\n",
    "\n",
    "        # 从模型中得到输出结果\n",
    "        output, h = net(inputs, h)\n",
    "        # 计算损失和执行反向传播\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm`有助于防止RNN / LSTM中的爆炸梯度问题。\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 损失数据\n",
    "        if counter % print_every == 0:\n",
    "            # 得到验证集损失\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, labels)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有几种方法可以测试神经网络能力：\n",
    "\n",
    "测试数据性能：首先，我们将看到我们训练的模型如何在我们上面定义的所有test_data上执行。 我们将计算测试数据的平均损失和准确度。\n",
    "\n",
    "对用户生成数据的推断：其次，我们将看看我们是否可以一次只输入一个示例评论（没有标签），并查看训练模型预测的内容。 查看这样的新用户输入数据并预测输出标签称为推理。\n",
    "但是，对于该示例的实际目的，第二选项不适用，因为任务是对所提供短语的同步收集进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.907\n",
      "Test accuracy: 0.630\n"
     ]
    }
   ],
   "source": [
    "# 获取测试集的损失和准确率\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "#在test data迭代\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # 得到预测的结果\n",
    "    output, h = net(inputs, h)\n",
    "    # 计算损失\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # 将输出概率转换为预测类\n",
    "    _, pred = torch.max(output,1)\n",
    "    \n",
    "    # 将预测值和真实label比较\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# -- stats! -- ##\n",
    "#平均的测试损失\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# 整个测试集的准确率\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
