{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:17.819876Z",
     "start_time": "2019-07-31T07:37:04.950914Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator, BucketIterator\n",
    "from torchtext import data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:17.864941Z",
     "start_time": "2019-07-31T07:37:17.824873Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_iters(batch_size=32, device=\"cpu\", data_path='data', vectors=None, use_tree=False):\n",
    "    if not use_tree:\n",
    "        #定义Field:声明如何处理数据\n",
    "        # Field使用include_lengths参数为True,可以在返回minibatch的时候同时返回一个表示每个句子的长度的list\n",
    "        TEXT = data.Field(batch_first=True, include_lengths=True, lower=True)  \n",
    "        LABEL = data.LabelField(batch_first=True)\n",
    "        TREE = None\n",
    "\n",
    "        fields = {'sentence1': ('premise', TEXT),\n",
    "                  'sentence2': ('hypothesis', TEXT),\n",
    "                  'gold_label': ('label', LABEL)}\n",
    "    else:\n",
    "        TEXT = data.Field(batch_first=True,\n",
    "                          lower=True,\n",
    "                          preprocessing=lambda parse: [t for t in parse if t not in ('(', ')')],\n",
    "                          include_lengths=True)\n",
    "        LABEL = data.LabelField(batch_first=True)\n",
    "        TREE = data.Field(preprocessing=lambda parse: ['reduce' if t == ')' else 'shift' for t in parse if t != '('],\n",
    "                          batch_first=True)\n",
    "\n",
    "        TREE.build_vocab([['reduce'], ['shift']])  #构建词表\n",
    "\n",
    "        fields = {'sentence1_binary_parse': [('premise', TEXT),\n",
    "                                             ('premise_transitions', TREE)],\n",
    "                  'sentence2_binary_parse': [('hypothesis', TEXT),\n",
    "                                             ('hypothesis_transitions', TREE)],\n",
    "                  'gold_label': ('label', LABEL)}\n",
    "    train_data, dev_data, test_data = data.TabularDataset.splits(\n",
    "        path=data_path,\n",
    "        train='snli_1.0_train.jsonl',\n",
    "        validation='snli_1.0_dev.jsonl',\n",
    "        test='snli_1.0_test.jsonl',\n",
    "        format='json',\n",
    "        fields=fields,\n",
    "        filter_pred=lambda ex: ex.label != '-'  # filter the example which label is '-'(means unlabeled)\n",
    "    )\n",
    "    if vectors is not None:\n",
    "        #unk_init表示的是对于未登录词的初始化方法，默认是使用全零进行初始化,这里用均值为0方差为1的正态分布去初始化\n",
    "        TEXT.build_vocab(train_data, vectors=vectors, unk_init=torch.Tensor.normal_)\n",
    "    else:\n",
    "        TEXT.build_vocab(train_data)\n",
    "    LABEL.build_vocab(dev_data)\n",
    "    \n",
    "    #相比于标准迭代器，会将类似长度的样本当做一批来处理，因为在文本处理中经常会需要将每一批样本长度补齐为当前批中\n",
    "    #最长序列的长度，因此当样本长度差别较大时，使用BucketIerator可以带来填充效率的提高。\n",
    "    train_iter, dev_iter = BucketIterator.splits(\n",
    "        (train_data, dev_data),\n",
    "        batch_sizes=(batch_size, batch_size),\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.premise) + len(x.hypothesis),  #sort_key是一个告诉迭代器如何对批处理中的元素进行排序的函数。\n",
    "        sort_within_batch=True,   #sort_within_batch=True告诉迭代器需要对批处理的内容进行排序\n",
    "        repeat=False,    #不重复多个epoches的迭代\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_iter = Iterator(test_data,      #这里为什么不用BucketIterator?\n",
    "                         batch_size=batch_size,\n",
    "                         device=device,\n",
    "                         sort=False,\n",
    "                         sort_within_batch=False,   ##sort_within_batch=False告诉迭代器不需要对批处理的内容进行排序\n",
    "                         repeat=False,\n",
    "                         shuffle=False)\n",
    "\n",
    "    return train_iter, dev_iter, test_iter, TEXT, LABEL, TREE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:18.070440Z",
     "start_time": "2019-07-31T07:37:17.868939Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于输入的词向量，首先使用 BILSTM来学习如何表示一个word以及上下文，\n",
    "\n",
    "即对原始的word embedding在当前的语境下重新编码，得到两个句子的新的词向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:18.265045Z",
     "start_time": "2019-07-31T07:37:18.075415Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, dropout_rate=0.1, layer_num=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        if layer_num == 1:\n",
    "            self.bilstm = nn.LSTM(input_size, hidden_size // 2, layer_num, batch_first=True, bidirectional=True)\n",
    "\n",
    "        else:\n",
    "            self.bilstm = nn.LSTM(input_size, hidden_size // 2, layer_num, batch_first=True, dropout=dropout_rate,\n",
    "                                  bidirectional=True)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for p in self.bilstm.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.normal_(p)   #使p服从正态分布\n",
    "                p.data.mul_(0.01)   #均值为0.01？？\n",
    "            else:\n",
    "                p.data.zero_()\n",
    "                # This is the range of indices for our forget gates for each LSTM cell\n",
    "                p.data[self.hidden_size // 2: self.hidden_size] = 1\n",
    "                \n",
    "    def forward(self, x, lens):\n",
    "        '''\n",
    "        :param x: (batch, seq_len, input_size)\n",
    "        :param lens: (batch, )\n",
    "        :return: (batch, seq_len, hidden_size)\n",
    "        '''\n",
    "        ordered_lens, index = lens.sort(descending=True)\n",
    "        ordered_x = x[index]\n",
    "    \n",
    "        #ordered_lens需要从大到小排序，ordered_x为已根据长度大小排好序，batch_first如果设置为true，则x的第一维为batch_size，第二维为seq_length，否则相反。 \n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(ordered_x, ordered_lens, batch_first=True)   #是打包序列\n",
    "        packed_output, _ = self.bilstm(packed_x)  #打包后的tensor\n",
    "        #解包，返回输出和每个的长度，输出的第一个维度是填充序列长度，但是由于使用打包填充序列，当填充标记是输入时，张量的值将全为零，且不需要用到\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)   \n",
    "\n",
    "        recover_index = index.argsort()   #将索引从小到大排列\n",
    "        recover_output = output[recover_index]\n",
    "        return recover_output    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-30T15:53:48.264262Z",
     "start_time": "2019-07-30T15:53:48.123648Z"
    }
   },
   "source": [
    "第二层先用Attention来提取前提与假设之间的关系，然后重构，以前提为例：\n",
    "\n",
    "其中，x1为前提，x2为假设"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:18.439149Z",
     "start_time": "2019-07-31T07:37:18.265045Z"
    }
   },
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_labels, embed_size, hidden_size, dropout_rate=0.1, layer_num=1,\n",
    "                 pretrained_embed=     None, freeze=False):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.pretrained_embed = pretrained_embed\n",
    "        if pretrained_embed is not None:\n",
    "            self.embed = nn.Embedding.from_pretrained(pretrained_embed, freeze)\n",
    "        else:\n",
    "            self.embed = nn.Embedding(vocab_size, embed_size)   #降维，\n",
    "        self.bilstm1 = BiLSTM(embed_size, hidden_size, dropout_rate, layer_num)\n",
    "        self.bilstm2 = BiLSTM(hidden_size, hidden_size, dropout_rate, layer_num)\n",
    "        self.fc1 = nn.Linear(4 * hidden_size, hidden_size)   #？\n",
    "        self.fc2 = nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.pretrained_embed is None:\n",
    "            nn.init.normal_(self.embed.weight)\n",
    "            self.embed.weight.data.mul_(0.01)\n",
    "        nn.init.normal_(self.fc1.weight)\n",
    "        self.fc1.weight.data.mul_(0.01)\n",
    "        nn.init.normal_(self.fc2.weight)\n",
    "        self.fc2.weight.data.mul_(0.01)\n",
    "        nn.init.normal_(self.fc3.weight)\n",
    "        self.fc3.weight.data.mul_(0.01)\n",
    "        \n",
    "    def soft_align_attention(self, x1, x1_lens, x2, x2_lens):\n",
    "        '''\n",
    "        local inference modeling\n",
    "        :param x1: (batch, seq1_len, hidden_size)\n",
    "        :param x1_lens: (batch, )\n",
    "        :param x2: (batch, seq2_len, hidden_size)\n",
    "        :param x2_lens: (batch, )\n",
    "        :return: x1_align (batch, seq1_len, hidden_size)\n",
    "                 x2_align (batch, seq2_len, hidden_size)\n",
    "        '''\n",
    "        seq1_len = x1.size(1)   #句子的长度等于列的大小\n",
    "        seq2_len = x2.size(1)   \n",
    "        batch_size = x1.size(0)   #batch大小等于行的大小\n",
    "        \n",
    "        #计算两个句子word之间的相似度\n",
    "        #torch.matmul是torch.mm的broadcast版本，x2.transpose(1,2)交换其维度一和维度二，使hidden_size与seq2_len交换位置\n",
    "        attention = torch.matmul(x1, x2.transpose(1, 2))  # (batch（个数）, seq1_len(行), seq2_len（列）)\n",
    "        #unsqueeze(1)在XX的第二维上增加一个维度\n",
    "        mask1 = torch.arange(seq1_len).expand(batch_size, seq1_len).to(x1.device) >= x1_lens.unsqueeze(\n",
    "            1)  # (batch, seq1_len), 1 means <pad>\n",
    "        mask2 = torch.arange(seq2_len).expand(batch_size, seq2_len).to(x1.device) >= x2_lens.unsqueeze(\n",
    "            1)  # (batch, seq2_len)\n",
    "        mask1 = mask1.float().masked_fill_(mask1, float('-inf'))\n",
    "        mask2 = mask2.float().masked_fill_(mask2, float('-inf'))\n",
    "        #weight2是x2的每个词对x1的归一化相关程度，即attention值。\n",
    "        weight2 = F.softmax(attention + mask2.unsqueeze(1), dim=-1)  # (batch, seq1_len, seq2_len),\n",
    "        #对假设x2进行加权求和，该值提取出了x2中与x1相关的部分；\n",
    "        x1_align = torch.matmul(weight2, x2)  # (batch, seq1_len, hidden_size)\n",
    "        weight1 = F.softmax(attention.transpose(1, 2) + mask1.unsqueeze(1), dim=-1)  # (batch, seq2_len, seq1_len)\n",
    "        x2_align = torch.matmul(weight1, x1)  # (batch, seq2_len, hidden_size)\n",
    "        return x1_align, x2_align\n",
    "    \n",
    "    def composition(self, x, lens):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x_compose = self.bilstm2(self.dropout(x), lens)  # (batch, seq_len, hidden_size)\n",
    "        p1 = F.avg_pool1d(x_compose.transpose(1, 2), x.size(1)).squeeze(-1)  # (batch, hidden_size)  #x.size(1)，x的列大小作为窗口大小\n",
    "        p2 = F.max_pool1d(x_compose.transpose(1, 2), x.size(1)).squeeze(-1)  # (batch, hidden_size)\n",
    "        return torch.cat([p1, p2], 1)  # (batch, hidden_size*2)，按维数1拼接，就是横着拼\n",
    "\n",
    "    def forward(self, x1, x1_lens, x2, x2_lens):\n",
    "        '''\n",
    "        :param x1: (batch, seq1_len)\n",
    "        :param x1_lens: (batch,)\n",
    "        :param x2: (batch, seq2_len)\n",
    "        :param x2_lens: (batch,)\n",
    "        :return: (batch, num_class)\n",
    "        '''\n",
    "        # Input encoding\n",
    "        embed1 = self.embed(x1)  # (batch, seq1_len, embed_size)\n",
    "        embed2 = self.embed(x2)  # (batch, seq2_len, embed_size)\n",
    "        #new_embed1是假设x1经过BiLSTM后的值\n",
    "        new_embed1 = self.bilstm1(self.dropout(embed1), x1_lens)  # (batch, seq1_len, hidden_size)\n",
    "        new_embed2 = self.bilstm1(self.dropout(embed2), x2_lens)  # (batch, seq2_len, hidden_size)\n",
    "\n",
    "        # Local inference collected over sequence\n",
    "        x1_align, x2_align = self.soft_align_attention(new_embed1, x1_lens, new_embed2, x2_lens)\n",
    "\n",
    "        # Enhancement of local inference information\n",
    "        #将四部分连接起来，用相减以及相乘来实现前提与假设的“交互推断”，文中说可以使得局部信息（如矛盾关系）更加明显；\n",
    "        x1_combined = torch.cat([new_embed1, x1_align, new_embed1 - x1_align, new_embed1 * x1_align],\n",
    "                                dim=-1)  # (batch, seq1_len, 4*hidden_size)\n",
    "        x2_combined = torch.cat([new_embed2, x2_align, new_embed2 - x2_align, new_embed2 * x2_align],\n",
    "                                 dim=-1)  # (batch, seq2_len, 4*hidden_size)\n",
    "\n",
    "        # Inference composition\n",
    "        x1_composed = self.composition(x1_combined, x1_lens)  # (batch, 2*hidden_size), v=[v_avg; v_max]\n",
    "        x2_composed = self.composition(x2_combined, x2_lens)  # (batch, 2*hidden_size)\n",
    "        composed = torch.cat([x1_composed, x2_composed], -1)  # (batch, 4*hidden_size)\n",
    "\n",
    "        # MLP classifier\n",
    "        out = self.fc3(self.dropout(torch.tanh(self.fc2(self.dropout(composed)))))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:18.588734Z",
     "start_time": "2019-07-31T07:37:18.439149Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import Vectors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:37:18.927230Z",
     "start_time": "2019-07-31T07:37:18.594733Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)  #为当前CPU设置种子用于生成随机数，使神经网络初始化\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:48:55.060340Z",
     "start_time": "2019-07-31T07:48:51.578074Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 600  # every LSTM's(forward and backward) hidden size is half of HIDDEN_SIZE\n",
    "EPOCHS = 6\n",
    "DROPOUT_RATE = 0.5\n",
    "LAYER_NUM = 1\n",
    "LEARNING_RATE = 4e-4\n",
    "PATIENCE = 5\n",
    "CLIP = 10\n",
    "EMBEDDING_SIZE = 300\n",
    "# vectors = None\n",
    "vectors = Vectors(name = '/home/xiyu/data/trainee/ZhangZhongmin/task3/data_task3/embeddings/glove.840B.300d.txt')\n",
    "freeze = False\n",
    "data_path = '/home/xiyu/data/trainee/Zhangbingbin/task3/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:48:59.381288Z",
     "start_time": "2019-07-31T07:48:59.133391Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_example(premise, hypothesis, label, TEXT, LABEL):\n",
    "    tqdm.write('Label: ' + LABEL.vocab.itos[label])   #itos:按照下标的顺序返回每一个单词\n",
    "    tqdm.write('premise: ' + ' '.join([TEXT.vocab.itos[i] for i in premise]))\n",
    "    tqdm.write('hypothesis: ' + ' '.join([TEXT.vocab.itos[i] for i in hypothesis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:48:59.656768Z",
     "start_time": "2019-07-31T07:48:59.651512Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    #返回model中的参数的总数目\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  #numel()返回数组中元素的个数,变量的requires_grad标记的运算就相当于or。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:49:00.211310Z",
     "start_time": "2019-07-31T07:49:00.172747Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(data_iter, name, epoch=None, use_cache=False):\n",
    "    if use_cache:\n",
    "        model.load_state_dict(torch.load('best_model.ckpt'))   #加载保存的部分参数\n",
    "    model.eval()  #让模型变成测试模式，不启用BatchNormalization和Dropout\n",
    "    correct_num = 0\n",
    "    err_num = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():   #当网络中tensor不需要梯度时，用torch.no_grad处理\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            premise, premise_lens = batch.premise\n",
    "            hypothesis, hypothesis_lens = batch.hypothesis\n",
    "            labels = batch.label\n",
    "\n",
    "            output = model(premise, premise_lens, hypothesis, hypothesis_lens)\n",
    "            #argmax(-1)表示，按行返回最大值索引，reshape(-1)表示数组会根据剩下的维度计算出数组的另一个shape属性\n",
    "            predicts = output.argmax(-1).reshape(-1) \n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            correct_num += (predicts == labels).sum().item()\n",
    "            err_num += (predicts != batch.label).sum().item()\n",
    "\n",
    "    acc = correct_num / (correct_num + err_num)\n",
    "    if epoch is not None:\n",
    "        tqdm.write(\n",
    "            \"Epoch: %d, %s Acc: %.3f, Loss %.3f\" % (epoch + 1, name, acc, total_loss))\n",
    "    else:\n",
    "        tqdm.write(\n",
    "            \"%s Acc: %.3f, Loss %.3f\" % (name, acc, total_loss))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-31T07:49:01.437975Z",
     "start_time": "2019-07-31T07:49:01.425774Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, dev_iter, loss_func, optimizer, epochs, patience=5, clip=5):\n",
    "    best_acc = -1\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_iter):\n",
    "            premise, premise_lens = batch.premise\n",
    "            hypothesis, hypothesis_lens = batch.hypothesis\n",
    "            labels = batch.label\n",
    "            # show_example(premise[0],hypothesis[0], labels[0], TEXT, LABEL)\n",
    "\n",
    "            model.zero_grad()   #反向传播前，需要将梯度初始化为0，防止梯度累加爆炸\n",
    "            output = model(premise, premise_lens, hypothesis, hypothesis_lens)\n",
    "            loss = loss_func(output, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "        tqdm.write(\"Epoch: %d, Train Loss: %d\" % (epoch + 1, total_loss))\n",
    "\n",
    "        acc = eval(dev_iter, \"Dev\", epoch)\n",
    "        if acc<best_acc:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            best_acc = acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.ckpt')\n",
    "        if patience_counter >= patience:\n",
    "            tqdm.write(\"Early stopping: patience limit reached, stopping...\")\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-31T07:49:03.567Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 23,358,603 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [16:00<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 11805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Dev Acc: 0.809, Loss 150.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [21:04<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Dev Acc: 0.838, Loss 131.182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [15:33<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 7685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Dev Acc: 0.855, Loss 120.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [15:20<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 7048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Dev Acc: 0.857, Loss 119.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [15:19<00:00, 19.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Dev Acc: 0.863, Loss 113.772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17168/17168 [15:20<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 6247\n",
      "Epoch: 6, Dev Acc: 0.865, Loss 111.899\n",
      "Test Acc: 0.858, Loss 114.639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8577972312703583"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, dev_iter, test_iter, TEXT, LABEL, _ = load_iters(BATCH_SIZE, device, data_path, vectors)\n",
    "\n",
    "model = ESIM(len(TEXT.vocab), len(LABEL.vocab.stoi),   #stoi 返回每一个单词与其对应的下标\n",
    "                 EMBEDDING_SIZE, HIDDEN_SIZE, DROPOUT_RATE, LAYER_NUM,\n",
    "                 TEXT.vocab.vectors, freeze).to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "train(train_iter, dev_iter, loss_func, optimizer, EPOCHS,PATIENCE, CLIP)\n",
    "eval(test_iter, \"Test\", use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
